<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Hausa AI Chatbot - Interactive Voice & Text</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700;800&display=swap');
        
        body {
            font-family: 'Inter', sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        }
        
        .chat-message {
            animation: fadeIn 0.3s ease-in;
        }
        
        @keyframes fadeIn {
            from { opacity: 0; transform: translateY(10px); }
            to { opacity: 1; transform: translateY(0); }
        }
        
        .recording {
            animation: pulse 1.5s infinite;
        }
        
        @keyframes pulse {
            0%, 100% { transform: scale(1); }
            50% { transform: scale(1.1); }
        }
        
        .wave {
            animation: wave 0.8s ease-in-out infinite;
        }
        
        @keyframes wave {
            0%, 100% { transform: scaleY(0.5); }
            50% { transform: scaleY(1.5); }
        }
        
        .typing-dot {
            animation: bounce 1.4s infinite;
        }
        
        .typing-dot:nth-child(2) { animation-delay: 0.2s; }
        .typing-dot:nth-child(3) { animation-delay: 0.4s; }
        
        @keyframes bounce {
            0%, 60%, 100% { transform: translateY(0); }
            30% { transform: translateY(-10px); }
        }
        
        .file-preview-item {
            animation: slideIn 0.3s ease-out;
        }
        
        @keyframes slideIn {
            from { opacity: 0; transform: translateX(-10px); }
            to { opacity: 1; transform: translateX(0); }
        }
    </style>
</head>
<body class="min-h-screen p-4">
    
    <!-- Main Container -->
    <div class="max-w-5xl mx-auto">
        
        <!-- Header -->
        <div class="bg-white rounded-t-2xl shadow-xl p-6 border-b-4 border-purple-600">
            <div class="flex items-center justify-between flex-wrap gap-4">
                <div class="flex items-center space-x-4">
                    <div class="bg-gradient-to-br from-purple-500 to-indigo-600 p-3 rounded-full">
                        <svg class="w-8 h-8 text-white" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M8 10h.01M12 10h.01M16 10h.01M9 16H5a2 2 0 01-2-2V6a2 2 0 012-2h14a2 2 0 012 2v8a2 2 0 01-2 2h-5l-5 5v-5z"></path>
                        </svg>
                    </div>
                    <div>
                        <h1 class="text-2xl font-bold text-gray-800">Hausa AI Chatbot</h1>
                        <p class="text-sm text-gray-600">Client-Side ‚Ä¢ OpenAI & Gemini Powered</p>
                    </div>
                    <span id="demo-badge" class="hidden text-xs font-semibold px-2 py-1 rounded-full bg-blue-100 text-blue-700 border border-blue-200">Demo Mode</span>
                    </div>
                </div>
                <div class="flex items-center space-x-3">
                    <button type="button" onclick="toggleSettings()" class="p-2 bg-gray-100 rounded-lg hover:bg-gray-200 transition">
                        <svg class="w-5 h-5 text-gray-600" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10.325 4.317c.426-1.756 2.924-1.756 3.35 0a1.724 1.724 0 002.573 1.066c1.543-.94 3.31.826 2.37 2.37a1.724 1.724 0 001.065 2.572c1.756.426 1.756 2.924 0 3.35a1.724 1.724 0 00-1.066 2.573c.94 1.543-.826 3.31-2.37 2.37a1.724 1.724 0 00-2.572 1.065c-.426 1.756-2.924 1.756-3.35 0a1.724 1.724 0 00-2.573-1.066c-1.543.94-3.31-.826-2.37-2.37a1.724 1.724 0 00-1.065-2.572c-1.756-.426-1.756-2.924 0-3.35a1.724 1.724 0 001.066-2.573c-.94-1.543.826-3.31 2.37-2.37.996.608 2.296.07 2.572-1.065z"></path>
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M15 12a3 3 0 11-6 0 3 3 0 016 0z"></path>
                        </svg>
                    </button>
                    <div class="flex items-center space-x-2" id="status-indicator">
                        <span class="relative flex h-3 w-3">
                            <span class="animate-ping absolute inline-flex h-full w-full rounded-full bg-red-400 opacity-75"></span>
                            <span class="relative inline-flex rounded-full h-3 w-3 bg-red-500"></span>
                        </span>
                        <span class="text-sm font-medium text-gray-600">Not Configured</span>
                    </div>
                    <div id="activity-indicator" class="text-xs px-2 py-1 rounded-full bg-gray-100 text-gray-700">
                        Idle
                    </div>
                    <div id="auto-listen-countdown" class="text-xs text-gray-500"></div>
                </div>
            </div>
        </div>

        <!-- Settings Panel (Hidden by default) -->
        <div id="settings-panel" class="hidden bg-white shadow-lg p-6 border-b border-gray-200">
            <h3 class="text-lg font-bold text-gray-800 mb-4">‚öôÔ∏è Configuration</h3>
            
            <div class="space-y-6">
                <!-- AI Text Provider -->
                <div class="space-y-4">
                    <h4 class="text-sm font-semibold text-gray-800">Text Model</h4>
                    <!-- Demo Mode -->
                    <div class="flex items-center justify-between p-3 rounded-lg border border-gray-200 bg-gray-50">
                        <div>
                            <p class="text-sm font-medium text-gray-800">Demo Mode (no API keys)</p>
                            <p class="text-xs text-gray-600">Runs locally with Hausa templates. Keys optional.</p>
                        </div>
                        <label class="inline-flex items-center cursor-pointer">
                            <input type="checkbox" id="demo-mode-toggle" class="sr-only" onchange="toggleDemoMode()">
                            <span class="w-11 h-6 bg-gray-200 rounded-full peer peer-checked:bg-purple-600 transition relative">
                                <span class="absolute left-1 top-1 w-4 h-4 bg-white rounded-full peer-checked:translate-x-5 transition"></span>
                            </span>
                        </label>
                    </div>
                    <!-- Provider Selection -->
                    <div>
                        <label class="block text-sm font-medium text-gray-700 mb-2">AI Provider</label>
                        <select id="provider-select" onchange="updateProviderSettings()" class="w-full px-4 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-purple-500 focus:border-transparent">
                            <option value="openai">OpenAI (GPT-3.5/GPT-4)</option>
                            <option value="gemini">Google Gemini Pro</option>
                        </select>
                    </div>

                    <!-- API Key Input -->
                    <div>
                        <label class="block text-sm font-medium text-gray-700 mb-2">
                            API Key <span class="text-red-500">*</span>
                        </label>
                        <input type="password" id="api-key-input" placeholder="Enter your API key" 
                               class="w-full px-4 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-purple-500 focus:border-transparent">
                        <p class="text-xs text-gray-500 mt-1">
                            üîí Stored locally in your browser only. Never sent to any server except the AI provider.
                        </p>
                    </div>

                    <!-- Model Selection -->
                    <div id="model-section">
                        <label class="block text-sm font-medium text-gray-700 mb-2">Model</label>
                        <select id="model-select" class="w-full px-4 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-purple-500 focus:border-transparent">
                            <option value="gpt-3.5-turbo">GPT-3.5 Turbo (Fast & Affordable)</option>
                            <option value="gpt-4">GPT-4 (More Capable)</option>
                            <option value="gpt-4-turbo-preview">GPT-4 Turbo (Latest)</option>
                        </select>
                    </div>
                </div>

                <!-- Speech Provider -->
                <div class="space-y-4">
                    <h4 class="text-sm font-semibold text-gray-800">Speech (Text-to-Speech)</h4>
                    <div class="grid md:grid-cols-2 gap-4">
                        <div>
                            <label class="block text-sm font-medium text-gray-700 mb-2">TTS Provider</label>
                            <select id="tts-provider-select" onchange="updateTTSSettings()" class="w-full px-4 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-purple-500 focus:border-transparent">
                                <option value="webspeech">Browser (Web Speech API)</option>
                                <option value="google-tts">Google Cloud TTS</option>
                                <option value="azure-tts">Azure Cognitive TTS</option>
                            </select>
                        </div>
                        <div>
                            <label class="block text-sm font-medium text-gray-700 mb-2">Voice</label>
                            <select id="tts-voice-select" class="w-full px-4 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-purple-500 focus:border-transparent"></select>
                            <p class="text-xs text-gray-500 mt-1">Hausa voices listed when supported.</p>
                        </div>
                    </div>
                    <div class="grid md:grid-cols-2 gap-4">
                        <div>
                            <label class="block text-sm font-medium text-gray-700 mb-2">Auto Listen Timeout (min)</label>
                            <input type="number" min="1" max="60" step="1" id="auto-listen-timeout-input" placeholder="5" class="w-full px-4 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-purple-500 focus:border-transparent">
                            <p class="text-xs text-gray-500 mt-1">Auto disables continuous conversation after this duration.</p>
                        </div>
                    </div>
                    <div id="tts-google-keys" class="hidden">
                        <label class="block text-sm font-medium text-gray-700 mb-2">Google TTS API Key</label>
                        <input type="password" id="tts-api-key-input" placeholder="Enter Google Cloud API key" class="w-full px-4 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-purple-500 focus:border-transparent">
                    </div>
                    <div id="tts-azure-keys" class="hidden grid md:grid-cols-2 gap-4">
                        <div>
                            <label class="block text-sm font-medium text-gray-700 mb-2">Azure Speech Key</label>
                            <input type="password" id="tts-azure-key-input" placeholder="Enter Azure Speech key" class="w-full px-4 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-purple-500 focus:border-transparent">
                        </div>
                        <div>
                            <label class="block text-sm font-medium text-gray-700 mb-2">Azure Region</label>
                            <input type="text" id="tts-azure-region-input" placeholder="e.g. eastus, westeurope" class="w-full px-4 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-purple-500 focus:border-transparent">
                        </div>
                    </div>
                </div>

                <!-- Speech Recognition (STT) -->
                <div class="space-y-4">
                    <h4 class="text-sm font-semibold text-gray-800">Speech Recognition (STT)</h4>
                    <div class="grid md:grid-cols-2 gap-4">
                        <div>
                            <label class="block text-sm font-medium text-gray-700 mb-2">STT Provider</label>
                            <select id="stt-provider-select" class="w-full px-4 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-purple-500 focus:border-transparent">
                                <option value="webspeech">Browser (Web Speech API)</option>
                                <option value="openai-whisper">OpenAI Whisper (uses OpenAI key)</option>
                            </select>
                            <p class="text-xs text-gray-500 mt-1">Whisper provides higher Hausa accuracy using your OpenAI key.</p>
                        </div>
                        <div>
                            <label class="block text-sm font-medium text-gray-700 mb-2">Language</label>
                            <select id="stt-language-select" class="w-full px-4 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-purple-500 focus:border-transparent">
                                <option value="ha">Hausa (ha)</option>
                                <option value="ha-NG">Hausa (Nigeria)</option>
                                <option value="en">English</option>
                            </select>
                        </div>
                    </div>
                </div>

                <!-- Multi-Modal Settings -->
                <div class="space-y-4">
                    <h4 class="text-sm font-semibold text-gray-800">Multi-Modal Settings</h4>
                    <div class="space-y-2">
                        <label class="flex items-center">
                            <input type="checkbox" id="enable-image-upload" checked class="mr-2 rounded">
                            <span class="text-sm">Enable Image Uploads</span>
                        </label>
                        <label class="flex items-center">
                            <input type="checkbox" id="enable-audio-upload" checked class="mr-2 rounded">
                            <span class="text-sm">Enable Audio Uploads</span>
                        </label>
                        <div class="grid grid-cols-2 gap-2">
                            <div>
                                <label class="text-xs text-gray-600">Max Image Size (MB)</label>
                                <input type="number" id="max-image-size" value="10" min="1" max="25" class="w-full px-2 py-1 text-sm border rounded">
                            </div>
                            <div>
                                <label class="text-xs text-gray-600">Max Audio Size (MB)</label>
                                <input type="number" id="max-audio-size" value="25" min="1" max="25" class="w-full px-2 py-1 text-sm border rounded">
                            </div>
                        </div>
                        <div>
                            <label class="text-xs text-gray-600">Vision Model (OpenAI)</label>
                            <select id="vision-model-select" class="w-full px-2 py-1 text-sm border rounded">
                                <option value="gpt-4-vision-preview">GPT-4 Vision Preview</option>
                                <option value="gpt-4-turbo">GPT-4 Turbo with Vision</option>
                            </select>
                        </div>
                    </div>
                </div>

                <!-- Actions -->
                <div class="flex flex-wrap gap-3 items-center">
                    <button type="button" onclick="saveConfiguration()" class="flex-1 px-4 py-2 bg-purple-600 text-white rounded-lg hover:bg-purple-700 font-medium transition">
                        Save Configuration
                    </button>
                    <button type="button" onclick="clearConfiguration()" class="px-4 py-2 bg-red-600 text-white rounded-lg hover:bg-red-700 font-medium transition">
                        Clear
                    </button>
                    <button type="button" onclick="testTTS()" class="px-4 py-2 bg-gray-100 text-gray-800 rounded-lg hover:bg-gray-200 font-medium transition">
                        üîä Test Voice
                    </button>
                </div>
            </div>

            <!-- Help Text -->
            <div class="mt-4 p-4 bg-blue-50 rounded-lg space-y-2">
                <p class="text-sm text-blue-800">
                    <strong>Need an API key for text models?</strong><br>
                    ‚Ä¢ OpenAI: <a href="https://platform.openai.com/api-keys" target="_blank" class="underline">Get one here</a><br>
                    ‚Ä¢ Gemini: <a href="https://makersuite.google.com/app/apikey" target="_blank" class="underline">Get one here</a>
                </p>
                <p class="text-sm text-blue-800">
                    <strong>Hausa Text-to-Speech (TTS) keys:</strong><br>
                    ‚Ä¢ Google Cloud TTS (ha-GH): <a href="https://console.cloud.google.com/marketplace/product/google/texttospeech.googleapis.com" target="_blank" class="underline">Enable API</a><br>
                    ‚Ä¢ Azure TTS (ha-NG): <a href="https://portal.azure.com/#view/Microsoft_Azure_ProjectOxford/SpeechServicesVoice" target="_blank" class="underline">Create resource</a><br>
                    Preset voices: Google ‚Äî ha-GH-Standard-A; Azure ‚Äî ha-NG-AishaNeural, ha-NG-AbdullahiNeural
                </p>
                <p class="text-sm text-blue-800">
                    <strong>Demo Mode:</strong> No keys required. Uses local Hausa templates and browser speech where available.
                </p>
            </div>
        </div>

        <!-- Chat Container -->
        <div class="bg-gray-50 shadow-xl" style="height: 500px; overflow-y: auto;" id="chat-container">
            <div class="p-6 space-y-4" id="chat-messages">
                <!-- Welcome Message -->
                <div class="chat-message flex justify-start">
                    <div class="bg-white rounded-lg shadow-md p-4 max-w-md">
                        <div class="flex items-start space-x-3">
                            <div class="bg-purple-100 rounded-full p-2">
                                <svg class="w-5 h-5 text-purple-600" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9.75 17L9 20l-1 1h8l-1-1-.75-3M3 13h18M5 17h14a2 2 0 002-2V5a2 2 0 00-2-2H5a2 2 0 00-2 2v10a2 2 0 002 2z"></path>
                                </svg>
                            </div>
                            <div>
                                <p class="text-sm font-medium text-gray-900 mb-1">Hausa AI Assistant</p>
                                <p class="text-gray-700" id="welcome-text">
                                    <strong>Sannu! Barka da zuwa.</strong><br>
                                    Welcome! I'm your Hausa AI assistant. I can help you practice Hausa, translate, or have conversations.<br><br>
                                    <em id="welcome-hint">‚öôÔ∏è Please configure your API key using the settings button above to get started.</em>
                                </p>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <!-- Input Area -->
        <div class="bg-white rounded-b-2xl shadow-xl p-4">
            <div class="flex items-end space-x-3">
                <!-- Voice Input Button -->
                <button id="voice-btn" onclick="toggleVoiceInput()" 
                        class="flex-shrink-0 bg-gradient-to-br from-red-500 to-pink-600 text-white p-3 rounded-full hover:from-red-600 hover:to-pink-700 transition-all transform hover:scale-105 shadow-lg"
                        title="Voice input (Experimental)">
                    <svg class="w-6 h-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                        <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 11a7 7 0 01-7 7m0 0a7 7 0 01-7-7m7 7v4m0 0H8m4 0h4m-4-8a3 3 0 01-3-3V5a3 3 0 116 0v6a3 3 0 01-3 3z"></path>
                    </svg>
                </button>

                <!-- File Attachment Button -->
                <button id="attach-btn" onclick="document.getElementById('file-input').click()"
                        class="flex-shrink-0 bg-gradient-to-br from-blue-500 to-cyan-600 text-white p-3 rounded-full hover:from-blue-600 hover:to-cyan-700 transition-all transform hover:scale-105 shadow-lg"
                        title="Attach image or audio">
                    <svg class="w-6 h-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                        <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M15.172 7l-6.586 6.586a2 2 0 102.828 2.828l6.414-6.586a4 4 0 00-5.656-5.656l-6.415 6.585a6 6 0 108.486 8.486L20.5 13"></path>
                    </svg>
                </button>
                <input type="file" id="file-input" class="hidden" accept="image/*,audio/*" multiple onchange="handleFileSelect(event)">

                <!-- Text Input -->
                <div class="flex-grow">
                    <textarea id="message-input" rows="2" placeholder="Type in Hausa or English..." 
                              class="w-full px-4 py-3 border border-gray-300 rounded-xl focus:ring-2 focus:ring-purple-500 focus:border-transparent resize-none"
                              onkeydown="handleKeyPress(event)"></textarea>
                </div>

                <!-- Send Button -->
                <button onclick="sendMessage()" id="send-btn"
                        class="flex-shrink-0 bg-gradient-to-br from-purple-600 to-indigo-700 text-white px-6 py-3 rounded-xl hover:from-purple-700 hover:to-indigo-800 transition-all transform hover:scale-105 shadow-lg">
                    <svg class="w-6 h-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                        <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 19l9 2-9-18-9 18 9-2zm0 0v-8"></path>
                    </svg>
                </button>
            </div>

            <!-- File Preview Container -->
            <div id="file-preview" class="hidden mt-3 flex flex-wrap gap-2">
                <!-- File preview items will be added here dynamically -->
            </div>

            <!-- Audio Visualization (hidden by default) -->
            <div id="audio-viz" class="hidden mt-4 flex justify-center items-center space-x-1">
                <div class="wave bg-purple-500 w-1 h-8 rounded-full"></div>
                <div class="wave bg-purple-500 w-1 h-12 rounded-full" style="animation-delay: 0.1s"></div>
                <div class="wave bg-purple-500 w-1 h-6 rounded-full" style="animation-delay: 0.2s"></div>
                <div class="wave bg-purple-500 w-1 h-10 rounded-full" style="animation-delay: 0.3s"></div>
                <div class="wave bg-purple-500 w-1 h-8 rounded-full" style="animation-delay: 0.4s"></div>
            </div>

            <!-- Status/Error Messages -->
            <div id="status-message" class="hidden mt-3 p-3 rounded-lg text-sm"></div>
        </div>

        <!-- Quick Actions -->
        <div class="mt-4 flex flex-wrap gap-2 justify-center">
            <button onclick="clearChat()" class="px-4 py-2 bg-white text-gray-700 rounded-lg shadow hover:shadow-md transition text-sm font-medium">
                üóëÔ∏è Clear Chat
            </button>
            <button onclick="exportChat()" class="px-4 py-2 bg-white text-gray-700 rounded-lg shadow hover:shadow-md transition text-sm font-medium">
                üíæ Export Chat
            </button>
            <button onclick="toggleVoiceResponses()" id="voice-toggle-btn" class="px-4 py-2 bg-white text-gray-700 rounded-lg shadow hover:shadow-md transition text-sm font-medium">
                üîä Speak Replies: Off
            </button>
            <button onclick="toggleAutoListen()" id="auto-listen-btn" class="px-4 py-2 bg-white text-gray-700 rounded-lg shadow hover:shadow-md transition text-sm font-medium">
                üéß Auto Listen: Off
            </button>
            <button onclick="stopAutoNow()" id="stop-auto-btn" class="px-4 py-2 bg-white text-red-600 rounded-lg border border-red-200 hover:bg-red-50 transition text-sm font-medium">
                ‚èπ Stop Auto
            </button>
        </div>

        <!-- Info Panel -->
        <div class="mt-6 bg-white rounded-2xl shadow-xl p-6">
            <h3 class="text-lg font-bold text-gray-800 mb-3">‚ú® Features</h3>
            <div class="grid md:grid-cols-3 gap-4 text-sm">
                <div class="flex items-start space-x-2">
                    <span class="text-2xl">üí¨</span>
                    <div>
                        <p class="font-semibold text-gray-800">Hausa Conversation</p>
                        <p class="text-gray-600">Natural language processing for Hausa</p>
                    </div>
                </div>
                <div class="flex items-start space-x-2">
                    <span class="text-2xl">üé§</span>
                    <div>
                        <p class="font-semibold text-gray-800">Voice Input</p>
                        <p class="text-gray-600">Speak in Hausa or English</p>
                    </div>
                </div>
                <div class="flex items-start space-x-2">
                    <span class="text-2xl">üìé</span>
                    <div>
                        <p class="font-semibold text-gray-800">Multi-Modal</p>
                        <p class="text-gray-600">Image & audio attachments with vision AI</p>
                    </div>
                </div>
                <div class="flex items-start space-x-2">
                    <span class="text-2xl">üîí</span>
                    <div>
                        <p class="font-semibold text-gray-800">Privacy First</p>
                        <p class="text-gray-600">Client-side processing</p>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <!-- Optional Hausa dataset for demo responses -->
    <script src="hausa_data.js" defer></script>

    <script>
        // Optional Hausa dataset
        // If static/hausa_data.js exists and defines window.HAUSA_DATA, we can enrich demo responses.
        // ================================
        // Configuration & State Management
        // ================================
        
        const CONFIG_KEY = 'hausa_chatbot_config';
        const CHAT_HISTORY_KEY = 'hausa_chatbot_history';
        
        let config = {
            provider: 'openai',
            apiKey: '',
            model: 'gpt-3.5-turbo',
            voiceEnabled: false,
            ttsProvider: 'webspeech',
            ttsApiKey: '',
            ttsRegion: '',
            ttsVoice: '',
            autoListen: false,
            autoListenTimeoutMinutes: 5,
            sttProvider: 'webspeech',
            sttLanguage: 'ha-NG',
            demoMode: true,
            imageUploadEnabled: true,
            audioUploadEnabled: true,
            maxImageSize: 10,
            maxAudioSize: 25,
            visionModel: 'gpt-4-vision-preview'
        };
        
        let chatHistory = [];
        let isRecording = false;
        let recognition = null;
        let attachedFiles = [];
        // Audio + STT/TTS runtime state
        let micStream = null;
        let audioCtx = null;
        let analyser = null;
        let micVizRAF = null;
        let currentAudio = null;
        let autoListenTimeoutId = null;
        let autoListenCountdownInterval = null;
        let mediaRecorder = null;
        let recordedChunks = [];
        
        // ================================
        // Initialization
        // ================================
        
        function init() {
            loadConfiguration();
            loadChatHistory();
            updateStatusIndicator();
            updateProviderSettings();
            updateTTSSettings();
            initSpeechRecognition();
            // Sync toggle buttons
            const speakBtn = document.getElementById('voice-toggle-btn');
            const listenBtn = document.getElementById('auto-listen-btn');
            if (speakBtn) speakBtn.textContent = config.voiceEnabled ? 'üîä Speak Replies: On' : 'üîä Speak Replies: Off';
            if (listenBtn) listenBtn.textContent = config.autoListen ? 'üéß Auto Listen: On' : 'üéß Auto Listen: Off';
            const demoToggle = document.getElementById('demo-mode-toggle');
            if (demoToggle) demoToggle.checked = !!config.demoMode;
            updateDemoBadge();
            setWelcomeText();
        }
        
        function loadConfiguration() {
            const saved = localStorage.getItem(CONFIG_KEY);
            if (saved) {
                try {
                    const savedConfig = JSON.parse(saved);
                    config = { ...config, ...savedConfig };
                    
                    // Populate UI
                    document.getElementById('provider-select').value = config.provider;
                    document.getElementById('api-key-input').value = config.apiKey;
                    document.getElementById('model-select').value = config.model;
                    // TTS
                    document.getElementById('tts-provider-select').value = config.ttsProvider || 'webspeech';
                    document.getElementById('tts-api-key-input').value = config.ttsApiKey || '';
                    document.getElementById('tts-azure-key-input').value = config.ttsApiKey || '';
                    document.getElementById('tts-azure-region-input').value = config.ttsRegion || '';
                    document.getElementById('auto-listen-timeout-input').value = config.autoListenTimeoutMinutes || 5;
                    // STT
                    document.getElementById('stt-provider-select').value = config.sttProvider || 'webspeech';
                    document.getElementById('stt-language-select').value = config.sttLanguage || 'ha-NG';
                    const demoToggle = document.getElementById('demo-mode-toggle');
                    if (demoToggle) demoToggle.checked = !!savedConfig.demoMode;
                    
                    // Multi-modal settings
                    if (savedConfig.imageUploadEnabled !== undefined) {
                        document.getElementById('enable-image-upload').checked = savedConfig.imageUploadEnabled;
                    }
                    if (savedConfig.audioUploadEnabled !== undefined) {
                        document.getElementById('enable-audio-upload').checked = savedConfig.audioUploadEnabled;
                    }
                    if (savedConfig.maxImageSize) {
                        document.getElementById('max-image-size').value = savedConfig.maxImageSize;
                    }
                    if (savedConfig.maxAudioSize) {
                        document.getElementById('max-audio-size').value = savedConfig.maxAudioSize;
                    }
                    if (savedConfig.visionModel) {
                        document.getElementById('vision-model-select').value = savedConfig.visionModel;
                    }
                    
                    updateTTSSettings();
                    if (config.ttsVoice) {
                        const ttsVoiceSelect = document.getElementById('tts-voice-select');
                        ttsVoiceSelect.value = config.ttsVoice;
                    }
                    
                    updateProviderSettings();
                } catch (e) {
                    console.error('Error loading config:', e);
                }
            }
        }
        
        function loadChatHistory() {
            const saved = localStorage.getItem(CHAT_HISTORY_KEY);
            if (saved) {
                try {
                    chatHistory = JSON.parse(saved);
                    // Restore messages to UI (without actual file data, just metadata)
                    chatHistory.forEach(msg => {
                        addMessageToUI(msg.content, msg.role, msg.files || []);
                    });
                } catch (e) {
                    console.error('Error loading chat history:', e);
                }
            }
        }
        
        function saveConfiguration() {
            config.provider = document.getElementById('provider-select').value;
            config.apiKey = document.getElementById('api-key-input').value.trim();
            config.model = document.getElementById('model-select').value;
            // TTS settings
            config.ttsProvider = document.getElementById('tts-provider-select').value;
            config.ttsVoice = document.getElementById('tts-voice-select').value || '';
            if (config.ttsProvider === 'google-tts') {
                config.ttsApiKey = document.getElementById('tts-api-key-input').value.trim();
            } else if (config.ttsProvider === 'azure-tts') {
                config.ttsApiKey = document.getElementById('tts-azure-key-input').value.trim();
                config.ttsRegion = document.getElementById('tts-azure-region-input').value.trim();
            } else {
                config.ttsApiKey = '';
                config.ttsRegion = '';
            }
            // Auto-listen timeout
            const timeoutVal = parseInt(document.getElementById('auto-listen-timeout-input').value || '5', 10);
            if (!isNaN(timeoutVal) && timeoutVal > 0 && timeoutVal <= 60) {
                config.autoListenTimeoutMinutes = timeoutVal;
            }
            // STT settings
            config.sttProvider = document.getElementById('stt-provider-select').value;
            config.sttLanguage = document.getElementById('stt-language-select').value;
            // Demo mode
            config.demoMode = !!document.getElementById('demo-mode-toggle').checked;
            
            // Multi-modal settings
            config.imageUploadEnabled = !!document.getElementById('enable-image-upload').checked;
            config.audioUploadEnabled = !!document.getElementById('enable-audio-upload').checked;
            const maxImageSize = parseInt(document.getElementById('max-image-size').value, 10);
            if (!isNaN(maxImageSize) && maxImageSize > 0) config.maxImageSize = maxImageSize;
            const maxAudioSize = parseInt(document.getElementById('max-audio-size').value, 10);
            if (!isNaN(maxAudioSize) && maxAudioSize > 0) config.maxAudioSize = maxAudioSize;
            config.visionModel = document.getElementById('vision-model-select').value;
            
            if (!config.demoMode && !config.apiKey) {
                showStatus('Please enter an API key for your selected text model (or enable Demo Mode)', 'error');
                return;
            }
            
            // Save to localStorage
            localStorage.setItem(CONFIG_KEY, JSON.stringify(config));
            
            updateStatusIndicator();
            showStatus('Configuration saved successfully! ‚úì', 'success');
            
            // Hide settings panel
            setTimeout(() => {
                toggleSettings();
            }, 1000);
            if (config.autoListen) scheduleAutoListenTimeout();
        }
        
        function clearConfiguration() {
            if (confirm('Clear all configuration and chat history?')) {
                localStorage.removeItem(CONFIG_KEY);
                localStorage.removeItem(CHAT_HISTORY_KEY);
                config = {
                    provider: 'openai',
                    apiKey: '',
                    model: 'gpt-3.5-turbo',
                    voiceEnabled: false
                };
                chatHistory = [];
                
                document.getElementById('api-key-input').value = '';
                clearChat();
                updateStatusIndicator();
                showStatus('Configuration cleared', 'success');
            }
        }
        
        function updateProviderSettings() {
            const provider = document.getElementById('provider-select').value;
            const modelSelect = document.getElementById('model-select');
            
            // Update model options based on provider
            if (provider === 'openai') {
                modelSelect.innerHTML = `
                    <option value="gpt-3.5-turbo">GPT-3.5 Turbo (Fast & Affordable)</option>
                    <option value="gpt-4">GPT-4 (Most Capable)</option>
                    <option value="gpt-4-turbo-preview">GPT-4 Turbo (Latest)</option>
                `;
            } else if (provider === 'gemini') {
                modelSelect.innerHTML = `
                    <option value="gemini-pro">Gemini Pro</option>
                `;
            }
        }

        function updateTTSSettings() {
            const provider = document.getElementById('tts-provider-select').value;
            const googleKeys = document.getElementById('tts-google-keys');
            const azureKeys = document.getElementById('tts-azure-keys');
            const voiceSelect = document.getElementById('tts-voice-select');
            
            googleKeys.classList.add('hidden');
            azureKeys.classList.add('hidden');
            voiceSelect.innerHTML = '';
            
            if (provider === 'webspeech') {
                voiceSelect.innerHTML = `<option value="">Browser default (tries Hausa if available)</option>`;
            } else if (provider === 'google-tts') {
                googleKeys.classList.remove('hidden');
                // Hausa voices available in Google Cloud TTS
                voiceSelect.innerHTML = `
                    <option value="ha-GH-Standard-A">ha-GH-Standard-A (Female)</option>
                    <option value="ha-GH-Standard-B">ha-GH-Standard-B (Male)</option>
                `;
            } else if (provider === 'azure-tts') {
                azureKeys.classList.remove('hidden');
                // Hausa neural voices in Azure
                voiceSelect.innerHTML = `
                    <option value="ha-NG-AishaNeural">ha-NG-AishaNeural (Female)</option>
                    <option value="ha-NG-AbdullahiNeural">ha-NG-AbdullahiNeural (Male)</option>
                `;
            }
        }
        
        function updateStatusIndicator() {
            const indicator = document.getElementById('status-indicator');
            const hasApiKey = config.apiKey && config.apiKey.length > 0;
            if (config.demoMode) {
                indicator.innerHTML = `
                    <span class="relative flex h-3 w-3">
                        <span class="animate-ping absolute inline-flex h-full w-full rounded-full bg-blue-400 opacity-75"></span>
                        <span class="relative inline-flex rounded-full h-3 w-3 bg-blue-500"></span>
                    </span>
                    <span class="text-sm font-medium text-gray-600">Demo Ready</span>
                `;
                return;
            }
            if (hasApiKey) {
                indicator.innerHTML = `
                    <span class="relative flex h-3 w-3">
                        <span class="animate-ping absolute inline-flex h-full w-full rounded-full bg-green-400 opacity-75"></span>
                        <span class="relative inline-flex rounded-full h-3 w-3 bg-green-500"></span>
                    </span>
                    <span class="text-sm font-medium text-gray-600">Ready</span>
                `;
            } else {
                indicator.innerHTML = `
                    <span class="relative flex h-3 w-3">
                        <span class="animate-ping absolute inline-flex h-full w-full rounded-full bg-red-400 opacity-75"></span>
                        <span class="relative inline-flex rounded-full h-3 w-3 bg-red-500"></span>
                    </span>
                    <span class="text-sm font-medium text-gray-600">Not Configured</span>
                `;
            }
        }
        
        // ================================
        // UI Controls
        // ================================
        
        function toggleSettings() {
            const panel = document.getElementById('settings-panel');
            panel.classList.toggle('hidden');
        }
        
        function toggleDemoMode() {
            const enabled = !!document.getElementById('demo-mode-toggle').checked;
            config.demoMode = enabled;
            localStorage.setItem(CONFIG_KEY, JSON.stringify(config));
            updateStatusIndicator();
            showStatus(enabled ? 'Demo Mode enabled ‚Äî no keys required' : 'Demo Mode disabled', 'success');
            updateDemoBadge();
            setWelcomeText();
        }
        
        function toggleVoiceResponses() {
            config.voiceEnabled = !config.voiceEnabled;
            const btn = document.getElementById('voice-toggle-btn');
            btn.textContent = config.voiceEnabled ? 'üîä Speak Replies: On' : 'üîä Speak Replies: Off';
            localStorage.setItem(CONFIG_KEY, JSON.stringify(config));
        }

        function toggleAutoListen() {
            config.autoListen = !config.autoListen;
            const btn = document.getElementById('auto-listen-btn');
            btn.textContent = config.autoListen ? 'üéß Auto Listen: On' : 'üéß Auto Listen: Off';
            localStorage.setItem(CONFIG_KEY, JSON.stringify(config));
            if (config.autoListen) {
                scheduleAutoListenTimeout();
            } else {
                clearAutoListenTimeout();
            }
        }

        function scheduleAutoListenTimeout() {
            clearAutoListenTimeout();
            const ms = (config.autoListenTimeoutMinutes || 5) * 60 * 1000;
            const endAt = Date.now() + ms;
            autoListenTimeoutId = setTimeout(() => {
                config.autoListen = false;
                localStorage.setItem(CONFIG_KEY, JSON.stringify(config));
                const btn = document.getElementById('auto-listen-btn');
                if (btn) btn.textContent = 'üéß Auto Listen: Off';
                showStatus('Auto Listen disabled (timeout reached)', 'info');
            }, ms);
            if (autoListenCountdownInterval) clearInterval(autoListenCountdownInterval);
            const countdownEl = document.getElementById('auto-listen-countdown');
            autoListenCountdownInterval = setInterval(() => {
                const remaining = Math.max(0, endAt - Date.now());
                const m = Math.floor(remaining / 60000);
                const s = Math.floor((remaining % 60000) / 1000);
                if (countdownEl) countdownEl.textContent = remaining > 0 ? `Auto ends in ${m}:${String(s).padStart(2,'0')}` : '';
                if (remaining <= 0) {
                    clearInterval(autoListenCountdownInterval);
                    autoListenCountdownInterval = null;
                }
            }, 500);
        }

        function clearAutoListenTimeout() {
            if (autoListenTimeoutId) {
                clearTimeout(autoListenTimeoutId);
                autoListenTimeoutId = null;
            }
            if (autoListenCountdownInterval) {
                clearInterval(autoListenCountdownInterval);
                autoListenCountdownInterval = null;
                const countdownEl = document.getElementById('auto-listen-countdown');
                if (countdownEl) countdownEl.textContent = '';
            }
        }
        
        function handleKeyPress(event) {
            if (event.key === 'Enter' && !event.shiftKey) {
                event.preventDefault();
                sendMessage();
            }
        }
        
        // ================================
        // File Handling Functions
        // ================================
        
        function validateFile(file, allowedTypes, maxSize) {
            if (!allowedTypes.includes(file.type.split('/')[0]) && !allowedTypes.some(t => file.type.includes(t))) {
                return { valid: false, error: 'File type not supported' };
            }
            if (file.size > maxSize * 1024 * 1024) {
                return { valid: false, error: `File size exceeds ${maxSize}MB limit` };
            }
            return { valid: true, error: null };
        }
        
        function getFileType(file) {
            if (file.type.startsWith('image/')) return 'image';
            if (file.type.startsWith('audio/')) return 'audio';
            return 'unsupported';
        }
        
        async function processImage(file) {
            return new Promise((resolve, reject) => {
                const reader = new FileReader();
                reader.onload = (e) => resolve(e.target.result);
                reader.onerror = reject;
                reader.readAsDataURL(file);
            });
        }
        
        async function compressImage(file, maxWidth = 1024) {
            return new Promise((resolve, reject) => {
                const reader = new FileReader();
                reader.onload = (e) => {
                    const img = new Image();
                    img.onload = () => {
                        const canvas = document.createElement('canvas');
                        let width = img.width;
                        let height = img.height;
                        
                        if (width > maxWidth) {
                            height = (height * maxWidth) / width;
                            width = maxWidth;
                        }
                        
                        canvas.width = width;
                        canvas.height = height;
                        const ctx = canvas.getContext('2d');
                        ctx.drawImage(img, 0, 0, width, height);
                        
                        canvas.toBlob((blob) => {
                            resolve(new File([blob], file.name, { type: file.type }));
                        }, file.type, 0.85);
                    };
                    img.onerror = reject;
                    img.src = e.target.result;
                };
                reader.onerror = reject;
                reader.readAsDataURL(file);
            });
        }
        
        async function processAudio(file) {
            // Return the file directly for FormData usage
            return file;
        }
        
        async function transcribeAudio(audioFile, fileName) {
            if (!config.apiKey || config.provider !== 'openai') {
                throw new Error('OpenAI API key required for audio transcription');
            }
            
            const formData = new FormData();
            formData.append('file', audioFile, fileName);
            formData.append('model', 'whisper-1');
            formData.append('language', 'ha');
            
            const response = await fetch('https://api.openai.com/v1/audio/transcriptions', {
                method: 'POST',
                headers: {
                    'Authorization': `Bearer ${config.apiKey}`
                },
                body: formData
            });
            
            if (!response.ok) {
                const error = await response.json();
                throw new Error(error.error?.message || 'Transcription failed');
            }
            
            const data = await response.json();
            return data.text;
        }
        
        async function handleFileSelect(event) {
            const files = Array.from(event.target.files);
            
            for (const file of files) {
                const fileType = getFileType(file);
                
                if (fileType === 'unsupported') {
                    showStatus(`‚ùå Unsupported file type: ${file.name}`, 'error');
                    continue;
                }
                
                const allowedTypes = fileType === 'image' ? ['image'] : ['audio'];
                const maxSize = fileType === 'image' ? config.maxImageSize : config.maxAudioSize;
                const validation = validateFile(file, allowedTypes, maxSize);
                
                if (!validation.valid) {
                    showStatus(`‚ùå ${file.name}: ${validation.error}`, 'error');
                    continue;
                }
                
                // Check if upload is enabled
                if (fileType === 'image' && !config.imageUploadEnabled) {
                    showStatus('‚ùå Image uploads are disabled', 'error');
                    continue;
                }
                if (fileType === 'audio' && !config.audioUploadEnabled) {
                    showStatus('‚ùå Audio uploads are disabled', 'error');
                    continue;
                }
                
                // Compress image if needed
                let processedFile = file;
                if (fileType === 'image' && file.size > 2 * 1024 * 1024) {
                    try {
                        processedFile = await compressImage(file);
                    } catch (e) {
                        console.error('Image compression failed:', e);
                    }
                }
                
                attachedFiles.push(processedFile);
            }
            
            // Clear file input
            event.target.value = '';
            
            displayFilePreview();
        }
        
        function removeAttachment(index) {
            attachedFiles.splice(index, 1);
            displayFilePreview();
        }
        
        function displayFilePreview() {
            const previewContainer = document.getElementById('file-preview');
            
            if (attachedFiles.length === 0) {
                previewContainer.classList.add('hidden');
                previewContainer.innerHTML = '';
                return;
            }
            
            previewContainer.classList.remove('hidden');
            previewContainer.innerHTML = '';
            
            attachedFiles.forEach((file, index) => {
                const fileType = getFileType(file);
                const previewItem = document.createElement('div');
                previewItem.className = 'file-preview-item flex items-center space-x-2 bg-gray-100 rounded-lg p-2 pr-3';
                
                let icon = '';
                if (fileType === 'image') {
                    icon = '<svg class="w-5 h-5 text-blue-500" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 16l4.586-4.586a2 2 0 012.828 0L16 16m-2-2l1.586-1.586a2 2 0 012.828 0L20 14m-6-6h.01M6 20h12a2 2 0 002-2V6a2 2 0 00-2-2H6a2 2 0 00-2 2v12a2 2 0 002 2z"></path></svg>';
                } else if (fileType === 'audio') {
                    icon = '<svg class="w-5 h-5 text-purple-500" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19V6l12-3v13M9 19c0 1.105-1.343 2-3 2s-3-.895-3-2 1.343-2 3-2 3 .895 3 2zm12-3c0 1.105-1.343 2-3 2s-3-.895-3-2 1.343-2 3-2 3 .895 3 2zM9 10l12-3"></path></svg>';
                }
                
                const fileSize = (file.size / 1024).toFixed(1);
                const fileName = file.name.length > 20 ? file.name.substring(0, 17) + '...' : file.name;
                
                previewItem.innerHTML = `
                    <div class="flex-shrink-0">${icon}</div>
                    <div class="flex-grow min-w-0">
                        <p class="text-xs font-medium text-gray-700 truncate">${fileName}</p>
                        <p class="text-xs text-gray-500">${fileSize} KB</p>
                    </div>
                    <button onclick="removeAttachment(${index})" class="flex-shrink-0 text-gray-400 hover:text-red-500">
                        <svg class="w-4 h-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M6 18L18 6M6 6l12 12"></path>
                        </svg>
                    </button>
                `;
                
                previewContainer.appendChild(previewItem);
            });
        }
        
        function clearAttachments() {
            attachedFiles = [];
            displayFilePreview();
        }
        
        function showStatus(message, type = 'info') {
            const statusDiv = document.getElementById('status-message');
            statusDiv.textContent = message;
            statusDiv.className = `mt-3 p-3 rounded-lg text-sm ${
                type === 'success' ? 'bg-green-100 text-green-800 border border-green-200' :
                type === 'error' ? 'bg-red-100 text-red-800 border border-red-200' :
                'bg-blue-100 text-blue-800 border border-blue-200'
            }`;
            statusDiv.classList.remove('hidden');
            
            setTimeout(() => {
                statusDiv.classList.add('hidden');
            }, 4000);
        }

        function setActivity(label) {
            const el = document.getElementById('activity-indicator');
            if (el) el.textContent = label || 'Idle';
        }

        function updateDemoBadge() {
            const badge = document.getElementById('demo-badge');
            if (!badge) return;
            if (config.demoMode) {
                badge.classList.remove('hidden');
            } else {
                badge.classList.add('hidden');
            }
        }

        function setWelcomeText() {
            const welcome = document.getElementById('welcome-text');
            const hint = document.getElementById('welcome-hint');
            if (!welcome || !hint) return;
            if (config.demoMode) {
                hint.textContent = 'üü¶ Demo Mode: running locally with Hausa templates. No keys required.';
            } else {
                hint.textContent = '‚öôÔ∏è Please configure your API key using the settings button above to get started.';
            }
        }
        
        function scrollToBottom() {
            const container = document.getElementById('chat-container');
            container.scrollTop = container.scrollHeight;
        }
        
        // ================================
        // Chat Functionality
        // ================================
        
        async function sendMessage() {
            const input = document.getElementById('message-input');
            const message = input.value.trim();
            
            // Allow sending if there's text OR attachments
            if (!message && attachedFiles.length === 0) return;
            
            // In demo mode, allow sending without API keys
            if (!config.demoMode && !config.apiKey) {
                showStatus('‚ö†Ô∏è Please configure your API key in settings or enable Demo Mode', 'error');
                toggleSettings();
                return;
            }
            
            // Process attachments
            const processedFiles = [];
            setActivity('Processing files');
            
            for (const file of attachedFiles) {
                const fileType = getFileType(file);
                
                try {
                    if (fileType === 'image') {
                        const base64 = await processImage(file);
                        processedFiles.push({
                            type: 'image',
                            data: base64,
                            name: file.name,
                            size: file.size
                        });
                    } else if (fileType === 'audio') {
                        const audioFile = await processAudio(file);
                        
                        // Try to transcribe audio if OpenAI is available
                        if (config.provider === 'openai' && !config.demoMode && config.apiKey) {
                            try {
                                setActivity('Transcribing audio');
                                const transcription = await transcribeAudio(audioFile, file.name);
                                processedFiles.push({
                                    type: 'audio',
                                    audioFile,
                                    transcription,
                                    name: file.name,
                                    size: file.size
                                });
                            } catch (e) {
                                console.error('Transcription error:', e);
                                processedFiles.push({
                                    type: 'audio',
                                    audioFile,
                                    name: file.name,
                                    size: file.size,
                                    error: e.message
                                });
                            }
                        } else {
                            processedFiles.push({
                                type: 'audio',
                                audioFile,
                                name: file.name,
                                size: file.size
                            });
                        }
                    }
                } catch (error) {
                    console.error('File processing error:', error);
                    showStatus(`‚ùå Error processing ${file.name}`, 'error');
                }
            }
            
            setActivity('Idle');
            
            // Add user message with files
            addMessage(message, 'user', processedFiles);
            input.value = '';
            clearAttachments();
            
            // Show typing indicator
            showTypingIndicator();
            
            try {
                // Get AI response (pass files for context)
                const response = await getAIResponse(message, processedFiles);
                
                // Remove typing indicator
                removeTypingIndicator();
                
                // Add AI response
                addMessage(response, 'assistant');
                
                // Speak response if enabled, then optionally auto-listen
                if (config.voiceEnabled) {
                    try { await speakText(response); } catch {}
                }
                if (config.autoListen && !isRecording) {
                    startVoiceInput();
                }
                
            } catch (error) {
                removeTypingIndicator();
                showStatus('Error: ' + error.message, 'error');
                console.error('Chat error:', error);
            }
        }
        
        function addMessage(text, role, files = []) {
            const messageData = { role, content: text };
            if (files && files.length > 0) {
                // Store file metadata only, not full data
                messageData.files = files.map(f => ({
                    type: f.type,
                    name: f.name,
                    size: f.size,
                    transcription: f.transcription
                }));
            }
            chatHistory.push(messageData);
            localStorage.setItem(CHAT_HISTORY_KEY, JSON.stringify(chatHistory));
            addMessageToUI(text, role, files);
        }
        
        function addMessageToUI(text, role, files = []) {
            const chatMessages = document.getElementById('chat-messages');
            const isUser = role === 'user';
            
            const messageDiv = document.createElement('div');
            messageDiv.className = `chat-message flex ${isUser ? 'justify-end' : 'justify-start'}`;
            
            const bgColor = isUser ? 'bg-gradient-to-br from-purple-600 to-indigo-700 text-white' : 'bg-white text-gray-700';
            const icon = isUser ? 
                '<svg class="w-5 h-5" fill="currentColor" viewBox="0 0 24 24"><path d="M12 12c2.21 0 4-1.79 4-4s-1.79-4-4-4-4 1.79-4 4 1.79 4 4 4zm0 2c-2.67 0-8 1.34-8 4v2h16v-2c0-2.66-5.33-4-8-4z"></path></svg>' :
                '<svg class="w-5 h-5" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9.75 17L9 20l-1 1h8l-1-1-.75-3M3 13h18M5 17h14a2 2 0 002-2V5a2 2 0 00-2-2H5a2 2 0 00-2 2v10a2 2 0 002 2z"></path></svg>';
            
            // Build file attachments HTML
            let filesHTML = '';
            if (files && files.length > 0) {
                filesHTML = '<div class="mt-2 space-y-2">';
                files.forEach(file => {
                    if (file.type === 'image' && file.data) {
                        filesHTML += `
                            <div class="rounded overflow-hidden border ${isUser ? 'border-white/30' : 'border-gray-300'}">
                                <img src="${file.data}" alt="${file.name}" class="max-w-full h-auto max-h-64 object-contain">
                                <div class="text-xs ${isUser ? 'bg-white/20 text-white' : 'bg-gray-100 text-gray-600'} px-2 py-1">${file.name}</div>
                            </div>
                        `;
                    } else if (file.type === 'audio') {
                        filesHTML += `
                            <div class="rounded p-2 ${isUser ? 'bg-white/20' : 'bg-gray-100'}">
                                <div class="flex items-center space-x-2 text-xs ${isUser ? 'text-white' : 'text-gray-700'}">
                                    <svg class="w-4 h-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                                        <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19V6l12-3v13M9 19c0 1.105-1.343 2-3 2s-3-.895-3-2 1.343-2 3-2 3 .895 3 2zm12-3c0 1.105-1.343 2-3 2s-3-.895-3-2 1.343-2 3-2 3 .895 3 2zM9 10l12-3"></path>
                                    </svg>
                                    <span class="font-medium">${file.name}</span>
                                </div>
                                ${file.transcription ? `<p class="text-xs ${isUser ? 'text-white/80' : 'text-gray-600'} mt-1 italic">Transcribed: "${file.transcription}"</p>` : ''}
                                ${file.error ? `<p class="text-xs text-red-300 mt-1">‚ö†Ô∏è ${file.error}</p>` : ''}
                            </div>
                        `;
                    }
                });
                filesHTML += '</div>';
            }
            
            messageDiv.innerHTML = `
                <div class="${bgColor} rounded-lg shadow-md p-4 max-w-md">
                    <div class="flex items-start space-x-3">
                        <div class="${isUser ? 'bg-white/20' : 'bg-purple-100'} rounded-full p-2">
                            ${icon}
                        </div>
                        <div class="flex-1">
                            <p class="text-sm font-medium mb-1">${isUser ? 'You' : 'AI Assistant'}</p>
                            ${filesHTML}
                            ${text ? `<p class="${isUser ? 'text-white' : 'text-gray-700'} whitespace-pre-wrap ${files && files.length > 0 ? 'mt-2' : ''}">${escapeHtml(text)}</p>` : ''}
                            ${!isUser && text ? `<button onclick="speakText(\`${text.replace(/`/g, '\\`')}\`)" class="mt-2 text-xs text-purple-600 hover:text-purple-800 flex items-center space-x-1">
                                <svg class="w-4 h-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M15.536 8.464a5 5 0 010 7.072m2.828-9.9a9 9 0 010 12.728"></path>
                                </svg>
                                <span>Play Audio</span>
                            </button>` : ''}
                        </div>
                    </div>
                </div>
            `;
            
            chatMessages.appendChild(messageDiv);
            scrollToBottom();
        }
        
        function showTypingIndicator() {
            const chatMessages = document.getElementById('chat-messages');
            const typingDiv = document.createElement('div');
            typingDiv.id = 'typing-indicator';
            typingDiv.className = 'chat-message flex justify-start';
            typingDiv.innerHTML = `
                <div class="bg-white rounded-lg shadow-md p-4">
                    <div class="flex space-x-2 items-center">
                        <div class="w-2 h-2 bg-purple-500 rounded-full typing-dot"></div>
                        <div class="w-2 h-2 bg-purple-500 rounded-full typing-dot"></div>
                        <div class="w-2 h-2 bg-purple-500 rounded-full typing-dot"></div>
                    </div>
                </div>
            `;
            chatMessages.appendChild(typingDiv);
            scrollToBottom();
        }
        
        function removeTypingIndicator() {
            const indicator = document.getElementById('typing-indicator');
            if (indicator) indicator.remove();
        }
        
        function clearChat() {
            if (confirm('Clear all chat messages?')) {
                chatHistory = [];
                localStorage.removeItem(CHAT_HISTORY_KEY);
                document.getElementById('chat-messages').innerHTML = '';
                
                // Add welcome message back
                init();
                showStatus('Chat cleared', 'success');
            }
        }
        
        function exportChat() {
            const text = chatHistory.map(msg => 
                `${msg.role.toUpperCase()}: ${msg.content}`
            ).join('\n\n');
            
            const blob = new Blob([text], { type: 'text/plain' });
            const url = URL.createObjectURL(blob);
            const a = document.createElement('a');
            a.href = url;
            a.download = `hausa-chat-${new Date().toISOString().split('T')[0]}.txt`;
            a.click();
            URL.revokeObjectURL(url);
            
            showStatus('Chat exported!', 'success');
        }
        
        // ================================
        // AI Integration
        // ================================
        
        async function getAIResponse(message, files = []) {
            setActivity('Thinking');
            if (config.demoMode) {
                const r = await getDemoResponse(message, files);
                setActivity('Idle');
                return r;
            }
            if (config.provider === 'openai') {
                const r = await getOpenAIResponse(message, files);
                setActivity('Idle');
                return r;
            } else if (config.provider === 'gemini') {
                const r = await getGeminiResponse(message, files);
                setActivity('Idle');
                return r;
            }
        }
        
        async function getOpenAIResponse(message, files = []) {
            const systemPrompt = `You are a helpful Hausa language assistant. You can communicate fluently in Hausa and help users learn the language, translate, and have conversations. Always be respectful and culturally appropriate.`;
            
            // Check if we have images to use vision model
            const hasImages = files.some(f => f.type === 'image');
            const model = hasImages ? config.visionModel : config.model;
            
            // Build messages with file context
            let userMessage = message;
            
            // Add transcribed audio to message with better formatting
            const audioTranscriptions = files
                .filter(f => f.type === 'audio' && f.transcription)
                .map((f, i) => `Audio ${i + 1}: "${f.transcription}"`);
            
            if (audioTranscriptions.length > 0) {
                userMessage += '\n\n[Audio transcriptions:\n' + audioTranscriptions.join('\n') + ']';
            }
            
            const messages = [
                { role: 'system', content: systemPrompt },
                ...chatHistory.slice(-10).map(msg => ({
                    role: msg.role,
                    content: msg.content
                }))
            ];
            
            // For vision models, format message with images
            if (hasImages) {
                const contentArray = [];
                if (userMessage) {
                    contentArray.push({ type: 'text', text: userMessage });
                }
                files.forEach(file => {
                    if (file.type === 'image' && file.data) {
                        contentArray.push({
                            type: 'image_url',
                            image_url: {
                                url: file.data
                            }
                        });
                    }
                });
                messages.push({ role: 'user', content: contentArray });
            } else {
                messages.push({ role: 'user', content: userMessage });
            }
            
            const response = await fetch('https://api.openai.com/v1/chat/completions', {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json',
                    'Authorization': `Bearer ${config.apiKey}`
                },
                body: JSON.stringify({
                    model: model,
                    messages: messages,
                    temperature: 0.7,
                    max_tokens: 500
                })
            });
            
            if (!response.ok) {
                const error = await response.json();
                throw new Error(error.error?.message || 'API request failed');
            }
            
            const data = await response.json();
            return data.choices[0].message.content;
        }
        
        async function getGeminiResponse(message, files = []) {
            const systemPrompt = `You are a helpful Hausa language assistant. You can communicate fluently in Hausa and help users learn the language, translate, and have conversations. Always be respectful and culturally appropriate.`;
            
            // Add file context to message with better formatting
            let enhancedMessage = message;
            if (files.length > 0) {
                const fileDescriptions = [];
                files.forEach((f, i) => {
                    if (f.type === 'image') {
                        fileDescriptions.push(`[Attached Image ${i + 1}: ${f.name}]`);
                    } else if (f.type === 'audio') {
                        if (f.transcription) {
                            fileDescriptions.push(`[Attached Audio ${i + 1} - Transcription: "${f.transcription}"]`);
                        } else {
                            fileDescriptions.push(`[Attached Audio ${i + 1}: ${f.name}]`);
                        }
                    }
                });
                if (fileDescriptions.length > 0) {
                    enhancedMessage = fileDescriptions.join('\n') + '\n\n' + (message || 'Please help me with these files.');
                }
            }
            
            // Build conversation context
            const context = chatHistory.slice(-10).map(msg => 
                `${msg.role === 'user' ? 'User' : 'Assistant'}: ${msg.content}`
            ).join('\n');
            
            const prompt = `${systemPrompt}\n\nConversation so far:\n${context}\n\nUser: ${enhancedMessage}\n\nAssistant:`;
            
            const response = await fetch(`https://generativelanguage.googleapis.com/v1beta/models/gemini-pro:generateContent?key=${config.apiKey}`, {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json'
                },
                body: JSON.stringify({
                    contents: [{
                        parts: [{ text: prompt }]
                    }],
                    generationConfig: {
                        temperature: 0.7,
                        maxOutputTokens: 500
                    }
                })
            });
            
            if (!response.ok) {
                const error = await response.json();
                throw new Error(error.error?.message || 'API request failed');
            }
            
            const data = await response.json();
            return data.candidates[0].content.parts[0].text;
        }
        
        async function getDemoResponse(message, files = []) {
            // Simple local Hausa assistant with templates and optional dictionary support
            const text = message.trim();
            const lower = text.toLowerCase();
            const now = new Date();
            const timeStr = now.toLocaleTimeString(undefined, { hour: '2-digit', minute: '2-digit' });
            
            // Handle file attachments
            if (files && files.length > 0) {
                const imageCount = files.filter(f => f.type === 'image').length;
                const audioCount = files.filter(f => f.type === 'audio').length;
                let fileResponse = 'Na ga ';
                if (imageCount > 0) fileResponse += `hoto${imageCount > 1 ? 'ci ' + imageCount : ''} `;
                if (audioCount > 0) fileResponse += `sauti${audioCount > 1 ? ' ' + audioCount : ''} `;
                fileResponse += `da ka aiko. `;
                
                // Mention transcriptions if any
                const transcriptions = files.filter(f => f.type === 'audio' && f.transcription);
                if (transcriptions.length > 0) {
                    fileResponse += `Na ji: "${transcriptions.map(f => f.transcription).join(', ')}". `;
                }
                
                if (text) {
                    return fileResponse + 'Ina iya taimaka maka game da wannan?';
                }
                return fileResponse + 'Yaya zan iya taimaka maka?';
            }
            
            // Greeting intents
            if (/\b(sannu|salama|barka|ina kwana|ina wuni|hello|hi|assalamu)/i.test(lower)) {
                return 'Sannu! Ina kwana? Ina iya taimaka maka a yau.';
            }
            // Time/date intents
            if (/\b(time|lokaci|agogo|date|kwanan wata)/i.test(lower)) {
                return `Lokaci yanzu: ${timeStr}.`;
            }
            // Help intent
            if (/\b(help|taimako|yadda|instructions)/i.test(lower)) {
                return 'Zaka iya tambayar fassara, karin magana, ko tattaunawa a Hausa. Misali: ‚ÄúFassara: How are you?‚Äù';
            }
            // Translation intent
            if (/\b(translate|fassara|translation)\b/i.test(lower) || /^fassara[:\-]/i.test(lower)) {
                const m = text.replace(/^fassara[:\-]?/i, '').replace(/\b(translate|fassara|translation)\b[:\-]?/i, '').trim();
                if (!m) return 'Ka rubuta kalma ko jumla da kake son fassara.';
                const demo = demoTranslate(m);
                return `Fassara (demo): ${demo}`;
            }
            // Dictionary lookup if dataset exists
            if (typeof window !== 'undefined' && window.HAUSA_DATA && window.HAUSA_DATA.dictionary) {
                const word = lower.split(/\s+/)[0];
                const entry = window.HAUSA_DATA.dictionary[word];
                if (entry) {
                    return `Kalma: ${word}\nMa'ana: ${entry.meaning || '‚Äî'}\nMisali: ${entry.example || '‚Äî'}`;
                }
            }
            // Fallback chat templates
            const replies = [
                'Nagode! Ka kara bayani, don in fahimta sosai.',
                'Lallai. Wanne bangare kake son mu tattauna?',
                'Ina jin Hausa da Turanci. Ka yi tambaya lafiya.',
                'Zan iya taimako da karin magana, fassara, ko hira mai sauki.'
            ];
            return replies[Math.floor(Math.random() * replies.length)];
        }
        
        function demoTranslate(text) {
            // Very naive placeholder translation: echo with note; if dictionary has entries, try map words
            if (typeof window !== 'undefined' && window.HAUSA_DATA && window.HAUSA_DATA.en2ha) {
                const words = text.toLowerCase().split(/\s+/);
                const mapped = words.map(w => window.HAUSA_DATA.en2ha[w] || w);
                return mapped.join(' ');
            }
            return `${text} (demo)`;
        }
        
        // ================================
        // Voice Features
        // ================================
        
        function initSpeechRecognition() {
            if ('webkitSpeechRecognition' in window || 'SpeechRecognition' in window) {
                const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
                recognition = new SpeechRecognition();
                recognition.continuous = false;
                recognition.interimResults = false;
                recognition.lang = (config.sttLanguage || 'ha-NG');
                recognition.onresult = async (event) => {
                    const transcript = event.results[0][0].transcript;
                    const input = document.getElementById('message-input');
                    input.value = transcript;
                    showStatus('Recognized: ' + transcript, 'success');
                    if (config.autoListen) {
                        await new Promise(r => setTimeout(r, 150));
                        sendMessage();
                    }
                };
                recognition.onerror = (event) => {
                    showStatus('Voice recognition error: ' + event.error, 'error');
                    stopVoiceInput();
                };
                recognition.onend = () => {
                    stopVoiceInput();
                };
            }
        }

        function toggleVoiceInput() {
            if (!isRecording) {
                startVoiceInput();
            } else {
                stopVoiceInput();
            }
        }

        async function startVoiceInput() {
            isRecording = true;
            document.getElementById('voice-btn').classList.add('recording');
            document.getElementById('audio-viz').classList.remove('hidden');
            startMicVisualizer().catch(() => {/* fallback */});
            beep();
            showStatus('üé§ Listening... Speak in Hausa or English', 'info');
            setActivity('Listening');
            if (config.sttProvider === 'openai-whisper') {
                await startWhisperRecording();
            } else {
                if (!recognition) {
                    showStatus('Speech recognition not supported in this browser', 'error');
                    stopVoiceInput();
                    return;
                }
                recognition.lang = (config.sttLanguage || 'ha-NG');
                recognition.start();
            }
        }

        function stopVoiceInput() {
            isRecording = false;
            try { if (recognition) recognition.stop(); } catch {}
            try { if (mediaRecorder && mediaRecorder.state === 'recording') mediaRecorder.stop(); } catch {}
            document.getElementById('voice-btn').classList.remove('recording');
            document.getElementById('audio-viz').classList.add('hidden');
            stopMicVisualizer();
            setActivity('Idle');
            beep(70, 500);
        }

        async function startWhisperRecording() {
            try {
                const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                micStream = stream;
                recordedChunks = [];
                mediaRecorder = new MediaRecorder(stream, { mimeType: 'audio/webm' });
                mediaRecorder.ondataavailable = (e) => { if (e.data.size > 0) recordedChunks.push(e.data); };
                mediaRecorder.onstop = async () => {
                    try {
                        const blob = new Blob(recordedChunks, { type: 'audio/webm' });
                        await transcribeWithWhisper(blob);
                    } catch (e) {
                        showStatus('Transcription failed: ' + (e.message || e), 'error');
                    }
                };
                mediaRecorder.start();
            } catch (e) {
                showStatus('Microphone error: ' + (e.message || e), 'error');
                stopVoiceInput();
            }
        }

        async function transcribeWithWhisper(blob) {
            if (!config.apiKey) {
                showStatus('OpenAI key required for Whisper', 'error');
                return;
            }
            const fd = new FormData();
            fd.append('file', blob, 'speech.webm');
            fd.append('model', 'whisper-1');
            const lang = (config.sttLanguage || 'ha-NG');
            fd.append('language', lang.toLowerCase().startsWith('ha') ? 'ha' : lang);
            const resp = await fetch('https://api.openai.com/v1/audio/transcriptions', {
                method: 'POST',
                headers: { 'Authorization': `Bearer ${config.apiKey}` },
                body: fd
            });
            if (!resp.ok) {
                const err = await safeJson(resp);
                throw new Error(err.error?.message || 'Whisper transcription failed');
            }
            const data = await resp.json();
            const transcript = data.text || '';
            const input = document.getElementById('message-input');
            input.value = transcript;
            showStatus('Recognized: ' + transcript, 'success');
            if (config.autoListen) {
                await new Promise(r => setTimeout(r, 150));
                sendMessage();
            }
        }
        
        async function speakText(text) {
            try {
                setActivity('Speaking');
                if (config.ttsProvider === 'google-tts') {
                    await speakWithGoogleTTS(text);
                    setActivity('Idle');
                    return;
                }
                if (config.ttsProvider === 'azure-tts') {
                    await speakWithAzureTTS(text);
                    setActivity('Idle');
                    return;
                }
                // Fallback: Web Speech API
                if ('speechSynthesis' in window) {
                    window.speechSynthesis.cancel();
                    await new Promise(resolve => {
                        const utterance = new SpeechSynthesisUtterance(text);
                        utterance.lang = 'ha-NG';
                        utterance.rate = 0.95;
                        utterance.pitch = 1.0;
                        const voices = window.speechSynthesis.getVoices();
                        const hausaVoice = voices.find(v => v.lang && v.lang.toLowerCase().startsWith('ha'));
                        if (hausaVoice) utterance.voice = hausaVoice;
                        utterance.onend = resolve;
                        window.speechSynthesis.speak(utterance);
                    });
                    showStatus('üîä Playing audio (browser voice)...', 'info');
                    setActivity('Idle');
                } else {
                    showStatus('Text-to-speech not supported in this browser', 'error');
                    setActivity('Idle');
                }
            } catch (e) {
                console.error('TTS error', e);
                showStatus('TTS error: ' + (e.message || e), 'error');
                setActivity('Idle');
            }
        }

        async function speakWithGoogleTTS(text) {
            if (!config.ttsApiKey) {
                showStatus('Google TTS API key required', 'error');
                return;
            }
            const voiceName = config.ttsVoice || 'ha-GH-Standard-A';
            const cacheKey = await ttsCacheKey('google', voiceName, 0.95, 0, text);
            const cached = await ttsCacheGet(cacheKey);
            if (cached) { await playAudioBlob(cached); showStatus('üîä Playing audio (cache)...', 'info'); return; }
            const body = {
                input: { text },
                voice: { languageCode: 'ha-GH', name: voiceName },
                audioConfig: { audioEncoding: 'MP3', speakingRate: 0.95, pitch: 0.0 }
            };
            const resp = await fetch(`https://texttospeech.googleapis.com/v1/text:synthesize?key=${encodeURIComponent(config.ttsApiKey)}`, {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify(body)
            });
            if (!resp.ok) {
                const err = await safeJson(resp);
                throw new Error(err.error?.message || 'Google TTS request failed');
            }
            const data = await resp.json();
            if (!data.audioContent) throw new Error('No audio content from Google TTS');
            const blob = base64ToBlob(data.audioContent, 'audio/mpeg');
            await ttsCachePut(cacheKey, blob);
            await playAudioBlob(blob);
            showStatus('üîä Playing audio (Google TTS)...', 'info');
        }

        async function speakWithAzureTTS(text) {
            if (!config.ttsApiKey || !config.ttsRegion) {
                showStatus('Azure Speech key and region required', 'error');
                return;
            }
            const tokenResp = await fetch(`https://${config.ttsRegion}.api.cognitive.microsoft.com/sts/v1.0/issueToken`, {
                method: 'POST',
                headers: { 'Ocp-Apim-Subscription-Key': config.ttsApiKey }
            });
            if (!tokenResp.ok) {
                throw new Error('Failed to obtain Azure token');
            }
            const token = await tokenResp.text();
            const voice = config.ttsVoice || 'ha-NG-AishaNeural';
            const ssml = `<?xml version="1.0" encoding="utf-8"?>
                <speak version="1.0" xml:lang="ha-NG">
                  <voice name="${voice}">
                    <prosody rate="0.95">${escapeXml(text)}</prosody>
                  </voice>
                </speak>`;
            const synthResp = await fetch(`https://${config.ttsRegion}.tts.speech.microsoft.com/cognitiveservices/v1`, {
                method: 'POST',
                headers: {
                    'Authorization': `Bearer ${token}`,
                    'Content-Type': 'application/ssml+xml',
                    'X-Microsoft-OutputFormat': 'audio-24khz-48kbitrate-mono-mp3',
                    'User-Agent': 'HausaChatbotClient'
                },
                body: ssml
            });
            if (!synthResp.ok) {
                throw new Error('Azure TTS synthesis failed');
            }
            const cacheKey = await ttsCacheKey('azure', voice, 0.95, 0, text);
            const cached = await ttsCacheGet(cacheKey);
            if (cached) { await playAudioBlob(cached); showStatus('üîä Playing audio (cache)...', 'info'); return; }
            const blob = await synthResp.blob();
            await ttsCachePut(cacheKey, blob);
            await playAudioBlob(blob);
            showStatus('üîä Playing audio (Azure TTS)...', 'info');
        }
        
        // ================================
        // Utility Functions
        // ================================
        
        function escapeHtml(text) {
            const div = document.createElement('div');
            div.textContent = text;
            return div.innerHTML;
        }

        function escapeXml(text) {
            return text
                .replace(/&/g, '&amp;')
                .replace(/</g, '&lt;')
                .replace(/>/g, '&gt;')
                .replace(/"/g, '&quot;')
                .replace(/'/g, '&apos;');
        }

        async function safeJson(resp) {
            try {
                return await resp.json();
            } catch {
                return {};
            }
        }

        function base64ToBlob(base64, contentType) {
            const byteChars = atob(base64);
            const byteNumbers = new Array(byteChars.length);
            for (let i = 0; i < byteChars.length; i++) byteNumbers[i] = byteChars.charCodeAt(i);
            const byteArray = new Uint8Array(byteNumbers);
            return new Blob([byteArray], { type: contentType });
        }

        async function sha256Hex(str) {
            const enc = new TextEncoder().encode(str);
            const hash = await crypto.subtle.digest('SHA-256', enc);
            const bytes = Array.from(new Uint8Array(hash));
            return bytes.map(b => b.toString(16).padStart(2, '0')).join('');
        }

        async function ttsCacheKey(provider, voice, rate, pitch, text) {
            const h = await sha256Hex(`${provider}|${voice}|${rate}|${pitch}|${text}`);
            return `${provider}:${voice}:${rate}:${pitch}:${h}`;
        }

        // IndexedDB cache for TTS
        let ttsDBPromise = null;
        function ttsDB() {
            if (ttsDBPromise) return ttsDBPromise;
            ttsDBPromise = new Promise((resolve, reject) => {
                const req = indexedDB.open('ttsCache', 1);
                req.onupgradeneeded = (e) => {
                    const db = req.result;
                    if (!db.objectStoreNames.contains('audio')) {
                        const store = db.createObjectStore('audio', { keyPath: 'key' });
                        store.createIndex('created', 'created');
                    }
                };
                req.onsuccess = () => resolve(req.result);
                req.onerror = () => reject(req.error);
            });
            return ttsDBPromise;
        }

        async function ttsCacheGet(key) {
            try {
                const db = await ttsDB();
                return await new Promise((resolve, reject) => {
                    const tx = db.transaction('audio', 'readonly');
                    const store = tx.objectStore('audio');
                    const getReq = store.get(key);
                    getReq.onsuccess = () => {
                        const rec = getReq.result;
                        resolve(rec ? rec.blob : null);
                    };
                    getReq.onerror = () => resolve(null);
                });
            } catch { return null; }
        }

        async function ttsCachePut(key, blob) {
            try {
                const db = await ttsDB();
                await new Promise((resolve, reject) => {
                    const tx = db.transaction('audio', 'readwrite');
                    const store = tx.objectStore('audio');
                    store.put({ key, blob, created: Date.now() });
                    tx.oncomplete = resolve;
                    tx.onerror = () => resolve();
                });
                // Opportunistic prune over 120 entries
                pruneTTSCache(120, 100).catch(() => {});
            } catch {}
        }

        async function pruneTTSCache(max = 120, keep = 100) {
            const db = await ttsDB();
            const count = await new Promise((resolve) => {
                const tx = db.transaction('audio', 'readonly');
                const store = tx.objectStore('audio');
                const req = store.count();
                req.onsuccess = () => resolve(req.result);
                req.onerror = () => resolve(0);
            });
            if (count <= max) return;
            const toDelete = count - keep;
            await new Promise((resolve) => {
                const tx = db.transaction('audio', 'readwrite');
                const store = tx.objectStore('audio');
                const idx = store.index('created');
                let removed = 0;
                idx.openCursor().onsuccess = (e) => {
                    const cursor = e.target.result;
                    if (cursor && removed < toDelete) {
                        store.delete(cursor.primaryKey);
                        removed++;
                        cursor.continue();
                    } else {
                        resolve();
                    }
                };
            });
        }

        function playAudioUrl(url) {
            return new Promise((resolve, reject) => {
                try { if (currentAudio) { currentAudio.pause(); currentAudio = null; } } catch {}
                const audio = new Audio(url);
                currentAudio = audio;
                audio.onended = () => { currentAudio = null; resolve(); };
                audio.onerror = (e) => { currentAudio = null; reject(e); };
                audio.play().catch(err => { currentAudio = null; reject(err); });
            });
        }

        function playAudioBlob(blob) {
            return new Promise((resolve, reject) => {
                const url = URL.createObjectURL(blob);
                try { if (currentAudio) { currentAudio.pause(); currentAudio = null; } } catch {}
                const audio = new Audio(url);
                currentAudio = audio;
                audio.onended = () => { currentAudio = null; URL.revokeObjectURL(url); resolve(); };
                audio.onerror = (e) => { currentAudio = null; URL.revokeObjectURL(url); reject(e); };
                audio.play().catch(err => { currentAudio = null; URL.revokeObjectURL(url); reject(err); });
            });
        }
        
        // ================================
        // Initialize on page load
        // ================================
        
        document.addEventListener('DOMContentLoaded', init);
        
        // Load voices for speech synthesis
        if ('speechSynthesis' in window) {
            speechSynthesis.onvoiceschanged = () => {
                speechSynthesis.getVoices();
            };
        }

        async function testTTS() {
            try {
                const sample = 'Sannu! Ina fata kana jin da…ói. Bari mu gwada murya.';
                await speakText(sample);
            } catch (e) {
                showStatus('Test TTS failed: ' + (e.message || e), 'error');
            }
        }

        function stopAllAudio() {
            try { if (currentAudio) { currentAudio.pause(); currentAudio.currentTime = 0; currentAudio = null; } } catch {}
            try { if ('speechSynthesis' in window) window.speechSynthesis.cancel(); } catch {}
        }

        function stopAutoNow() {
            stopAllAudio();
            stopVoiceInput();
            config.autoListen = false;
            localStorage.setItem(CONFIG_KEY, JSON.stringify(config));
            const btn = document.getElementById('auto-listen-btn');
            if (btn) btn.textContent = 'üéß Auto Listen: Off';
            clearAutoListenTimeout();
            showStatus('Stopped auto conversation and audio', 'info');
        }

        async function startMicVisualizer() {
            try {
                if (!navigator.mediaDevices?.getUserMedia) throw new Error('getUserMedia not supported');
                micStream = await navigator.mediaDevices.getUserMedia({ audio: true });
                audioCtx = new (window.AudioContext || window.webkitAudioContext)();
                const source = audioCtx.createMediaStreamSource(micStream);
                analyser = audioCtx.createAnalyser();
                analyser.fftSize = 256;
                source.connect(analyser);
                const dataArray = new Uint8Array(analyser.frequencyBinCount);
                const bars = Array.from(document.querySelectorAll('#audio-viz .wave'));
                const animate = () => {
                    if (!analyser) return;
                    analyser.getByteTimeDomainData(dataArray);
                    let sum = 0;
                    for (let i = 0; i < dataArray.length; i++) {
                        const v = (dataArray[i] - 128) / 128; // -1..1
                        sum += Math.abs(v);
                    }
                    const avg = sum / dataArray.length; // 0..1
                    const height = Math.max(6, Math.min(24, Math.round(avg * 80))); // px
                    bars.forEach((el, idx) => {
                        const variance = (idx % 3) * 4;
                        el.style.height = (height + variance) + 'px';
                    });
                    micVizRAF = requestAnimationFrame(animate);
                };
                micVizRAF = requestAnimationFrame(animate);
            } catch (e) {
                // Silently ignore; CSS animation will show
            }
        }

        function stopMicVisualizer() {
            try {
                if (micVizRAF) cancelAnimationFrame(micVizRAF);
                micVizRAF = null;
                if (analyser && analyser.disconnect) analyser.disconnect();
                analyser = null;
                if (audioCtx && audioCtx.close) audioCtx.close();
                audioCtx = null;
                if (micStream) {
                    micStream.getTracks().forEach(t => t.stop());
                }
                micStream = null;
            } catch {}
        }

        function beep(duration = 90, frequency = 720) {
            try {
                const ctx = new (window.AudioContext || window.webkitAudioContext)();
                const o = ctx.createOscillator();
                const g = ctx.createGain();
                o.connect(g);
                g.connect(ctx.destination);
                o.type = 'sine';
                o.frequency.value = frequency;
                g.gain.setValueAtTime(0.0001, ctx.currentTime);
                g.gain.exponentialRampToValueAtTime(0.2, ctx.currentTime + 0.02);
                g.gain.exponentialRampToValueAtTime(0.0001, ctx.currentTime + duration/1000);
                o.start();
                o.stop(ctx.currentTime + duration/1000 + 0.01);
            } catch {}
        }

        // Keyboard shortcuts
        document.addEventListener('keydown', (e) => {
            const tag = (document.activeElement && document.activeElement.tagName) || '';
            const inInput = tag === 'INPUT' || tag === 'TEXTAREA';
            if (inInput) return;
            if (e.key === 'm' || e.key === 'M') { e.preventDefault(); toggleVoiceInput(); }
            if (e.key === 'p' || e.key === 'P') { e.preventDefault(); stopAllAudio(); }
            if (e.key === 's' || e.key === 'S') { e.preventDefault(); stopAutoNow(); }
            if (e.key === 'v' || e.key === 'V') { e.preventDefault(); toggleVoiceResponses(); }
            if (e.key === 'l' || e.key === 'L') { e.preventDefault(); toggleAutoListen(); }
        });
    </script>
 </body>
 </html>
